* AutoEncoder(MNIST手写数字集)
** 实验目的
运用自编码器重构输出（keras框架下实现）
** 理论基础
1. 自编码器（Autoencoder, AE）
   1. 自编码器是一种无监督的数据维度压缩和数据特征表达方法，利用反向传播算法使得输出值等于输入值的神经网络，先将输入压缩成潜在空间表征，然后通过这种表征来重构输出。
      #+CAPTION: (图片标题)
      [[./img/autoencoder/thesis/AE_1.png]]
      自编码器由两部分组成：
      编码器（encoder）:这部分能将输入压缩成潜在空间表征，可以用编码函数h=f(x)表示。
      解码器（decoder）:这部分重构来自潜在空间表征的输入，可以用解码函数r=g(h)表示。
      因此，整个自编码器可以用函数g(f(x))=r来描述，其中输出r与原始输入x相近。h=f(x)表示编码器，r=g(h)=g(f(x))表示解码器，自编码的目标便是优化损失函数L(x,g(f(x))，也就是减小图中的Error。
   2. 自编码器是前馈神经网络的一种，最开始主要用于数据的降维以及特征的抽取，随着技术的不断发展，现在也被用于生成模型中，可用来生成图片等。前馈神经网络是有监督学习，其需要大量的标注数据。自编码器是无监督学习，数据不需要标注因此较容易收集。前馈神经网络在训练时主要关注的是输出层的数据以及错误率，而自编码的应用可能更多的关注中间隐层的结果。
   3. 在普通的自编码器中，输入和输出是完全相同的，因此输出没有什么应用价值，所以我们希望利用中间隐层的结果，比如，可以将其作为特征提取的结果、利用中间隐层获取最有用的特性等。但是如果只使用普通的自编码器会面临什么问题呢？比如，输入层和输出层的维度都是5，中间隐层的维度也是5，那么使用相同的输入和输出来不断优化隐层参数，最终得到的参数可能是：x1−>a1，x2−>a2，… 的参数为1，其余参数为0。也就是说，中间隐层的参数只是完全将输入记忆下来，并在输出时将其记忆的内容完全输出即可，神经网络在做恒等映射，产生数据过拟合。
      [[./img/autoencoder/thesis/AE_2.png]] 如图是隐层单元数等于输入维度的情况，如果是隐层单元数大于输入维度也会发生类似的情况，即当隐层单元数大于等于输入维度时，网络可以采用完全记忆的方式。虽然这种方式在训练时精度很高，但是复制的输出无实际意义。因此，往往给隐层加一些约束，如限制隐藏单元数、添加正则化等。
2. 栈式自编码器（Stack Autoencoder）
   1. 栈式自编码器又称为深度自编码器，其训练过程和深度神经网络有所区别，下面是基于栈式自编码器的分类问题的训练过程（图片来自台大李宏毅老师的PPT）：
      [[./img/autoencoder/thesis/Stack_AE_1.png]]
   2. 训练过程：首先，训练784->1000->784的自编码器，而后已经固定已经训练好的参数和1000维的结果，训练第二个自编码器：1000->1000->1000，而后固定已经训练好的参数和训练的中间层结果，训练第三个自编码器：1000->500->1000，固定参数和中间隐层的结果。此时，前3层的参数已经训练完毕，此时，最后一层接一个分类器，将整体网络使用反向传播进行训练，对参数进行微调。这便是使用栈式自编码器进行分类的整体过程。（encoder和decoder的参数可以是对称的，也可以是非对称的）
   3. 栈式自编码器增加隐层可以学到更复杂的编码，每一层可以学习到不同的信息维度。若层数太深，encoder过于强大，可以将学习将输入映射为任意数（然后decoder学习其逆映射）。这一编码器可以很好的重建数据，但并没有在这一过程中学到有用的数据表示。
3. 稀疏自编码器（Sparse Autoencoder）
   1. 稀疏自编码器是加入正则化的自编码器，其未限制网络接收数据的能力，即不限制隐藏层的单元数。
      所谓稀疏性限制是指：若激活函数是sigmoid，则当神经元的输出接近于1的时候认为神经元被激活，输出接近于0的时候认为神经元被抑制。使得大部分神经元别抑制的限制叫做稀疏性限制。若激活函数是tanh，则当神经元的输出接近于-1的时候认为神经元是被抑制的。
      [[./img/autoencoder/thesis/Sparse_AE_1.png]]
      如上图所示，浅色的神经元表示被抑制的神经元，深色的神经元表示被激活的神经元。通过稀疏自编码器，没有限制隐藏层的单元数，但防止了网络过度记忆的情况。
      稀疏自编码器损失函数的基本表示形式如下：
      [[./img/autoencoder/thesis/Sparse_AE_2.png]]
      其中g(h)g(h)是解码器的输出，通常h是编码器的输出，即h=f(x)。
   2. 损失函数和BP函数推导（暂略）
   3. 稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。
      [[./img/autoencoder/thesis/Sparse_AE_3.png]]
      上图所述过程不是一次训练的，可以看到上面只有编码器没有解码器，因此其训练过程是自编码器先使用数据训练参数，然后保留编码器，将解码器删除并在后面接一个分类器，并使用损失函数来训练参数已达到最后效果。
4. 去噪自编码器（Denoising Autoencoder）
   1. 去噪自编码器是一类接受损失数据作为输入，并训练来预测原始未被损坏的数据作为输出的自编码器。
      [[./img/autoencoder/thesis/Denoising_AE_1.png]]
   2. 训练过程：引入一个损坏过程 C(\tilde{x}|x)，这个条件分布代表给定数据样本x产生损坏样本\tilde{x}的概率。自编码器学习重构分布p_{reconstruct}(x|\tilde{x}):
      从训练数据中采一个训练样本x
      从C(\tilde{x}|X=x)采一个损坏样本\tilde{x} 
      将(\tilde{x}, x)作为训练样本来估计自编码器的重构分布p_{reconstruct}(x|\tilde{x})=p_{decoder}(x|h)，其中h是编码器f(\tilde{x})的输出，p_{decoder}p根据解码函数g(h)定义。
      去噪自编码器中作者给出的直观解释是：和人体感官系统类似，比如人的眼睛看物体时，如果物体的某一小部分被遮住了，人依然能够将其识别出来，所以去噪自编码器就是破坏输入后，使得算法学习到的参数仍然可以还原图片。
   3. 普通的自编码器的本质是学一个相等函数，即输入和输出是同一个内容，这种相等函数的缺点便是当测试样本和训练样本不符合同一个分布时，在测试集上效果不好，而去噪自编码器可以很好地解决这个问题。
      欠完备自编码器限制学习容量，而去噪自编码器允许学习容量很高，同时防止在编码器和解码器学习一个无用的恒等函数。
      经过了加入噪声并进行降噪的训练过程，能够强迫网络学习到更加鲁棒的不变性特征，获得输入的更有效的表达。
5. 卷积自编码器（convolutional Autoencoder）
   1. 卷积自编码器和普通自编码器的区别在于其encoder和decoder都是卷积神经网络，相应的，encoder使用的是卷积操作和池化操作，而decoder中使用的反卷积操作和反卷积操作。（关于卷积、反卷积、池化和反池化的内容暂略）
** 数据来源
MNIST手写数字集
** 实验步骤
1. 自编码器（Autoencoder, AE）
   Keras封装的比较厉害，这里是最简单的自编码器，其输入维度是28*28=784，中间单隐层的维度是2，使用的激活函数是Relu，返回encoder和autoencoder。encoder部分可以用于降维后的可视化，或者降维之后接分类等，autoencoder可以用来生成图片等。
   结构见图如下：
   [[./img/autoencoder/thesis/AE_3.png]]
2. 栈式自编码器（Stack Autoencoder）
   栈式自编码器相当于深度网络的过程，主要注意维度对应即可，另外，这里设置的encoder和decoder的维度是对称的。
   其架构图如下：
   [[./img/autoencoder/thesis/Stack_AE_2.png]]
3. 稀疏自编码器（Sparse Autoencoder）
   以多层的自编码器举例，单隐层的同样适用，主要是在第一层加一个正则化项，activity_regularizer=regularizers.l1(10e-6)说明加入的是L1正则化项，10e-6是正则化项系数。
   其架构如下：
   [[./img/autoencoder/thesis/Sparse_AE_4.png]]
4. 去噪自编码器（Denoising Autoencoder）
   去噪自编码器主要是对输入添加噪声，所以训练过程是不需要改变的，只需要改变输入和输出。
   上述便是对输入添加噪声的过程，NOISE_FACTOR * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)便是添加的噪声。 np.clip()是截取函数，将数值限制在0~1之间。
   其架构如下：
   [[./img/autoencoder/thesis/Denoising_AE_2.png]]
5. 卷积自编码器（convolutional Autoencoder）
   在Keras编码中，反卷积的实现代码便是卷积操作。UpSampling2D()实现的是反平均卷积的操作。 
   代码架构图如下：
   [[./img/autoencoder/thesis/Conv_AE.png]]
** 结果分析与讨论
1. 自编码器（Autoencoder, AE）
   Encoder结果的可视化如图：
   [[./img/autoencoder/MNIST/AE_Output_visualization.png]]
   上图中不同表示表示不同的数字，由图可知，自编码器降维之后的结果并不能很好地表示10个数字。
   AutoEncoder还原之后的图片和原图片对比如下：
   [[./img/autoencoder/MNIST/AE_restruction.png]]
   上图说明，autoencoder的生成结果不是很清晰。
2. 栈式自编码器（Stack Autoencoder）
   Encoder结果的可视化如图：
   [[./img/autoencoder/MNIST/Stack_AE_Output_visualization.png]]
   上图中不同表示表示不同的数字，由图可知，栈式自编码器的效果相比较普通自编码器好很多，这里基本能将10个分类全部分开。
   AutoEncoder还原之后的图片和原图片对比如下：
   [[./img/autoencoder/MNIST/Stack_AE_restruction.png]]
3. 稀疏自编码器（Sparse Autoencoder）
   Encoder结果的可视化如图：
   [[./img/autoencoder/MNIST/Sparse_AE_Output_visualization.png]]
   上图中不同颜色表示不同的数字，由图可知，这个编码器的分类效果还可以，比自编码器好很多，但作用不大，大部分作用需要归功于栈式自编码器。
   AutoEncoder还原之后的图片和原图片对比如下：
   [[./img/autoencoder/MNIST/Stack_AE_restruction.png]]
4. 去噪自编码器（Denoising Autoencoder）
   Encoder结果的可视化如图：
   [[./img/autoencoder/MNIST/Denoising_AE_Output_visualization.png]]
   上图中不同表示表示不同的数字，这里不是很直观，看下面的图片对比:
   [[./img/autoencoder/MNIST/Denoising_AE_add_noise.png]]
   上图是添加噪声的效果对比，第一行表示原数据，第二行表示噪声处理过后的数据。
   AutoEncoder还原之后的图片和原图片对比如下：
   [[./img/autoencoder/MNIST/Denoising_AE_restruction.png]]
   上图根据噪声数据还原图片的对比，第一行表示噪声处理过后的数据，第二行表示去噪自编码器decoder还原之后的结果，上图可看出去噪自编码器的效果不错。
5. 卷积自编码器（convolutional Autoencoder）
   AutoEncoder还原之后的图片和原图片对比如下：
   [[./img/autoencoder/MNIST/Conv_AE_restruction.png]]
   上图根据原图片和生成图片的对比，第一行表示原图片，第二行表示卷积自编码器decoder还原之后的结果，上图可看出效果不错。
** 结论
实验中用到的几种自编码器的变形对于不同数字的表示效果明显好于普通自编码器，基本能将10个分类分开；而且重构生成结果也较好。
以下效果需要着重考虑：
1. 可比性（不同自编码器之间以及其对火焰数据的适用程度）
2. 适用性（实验逻辑性出发点即对应要解决的具体问题和希望提升的效果）
* CNNmatching模型提取火焰信息
** 实验目的
利用深度学习中CNN神经网络对图片进行匹配的模型，对火焰图像进行处理匹配特征点。
** 理论基础
卷积神经网络(Convolutional Neural Networks, CNN)由纽约大学的Yann　LeCun于1998年提出，CNN中层次之间的紧密联系和空间信息使得其特别适用于图像的处理和理解，并且能够自动的从图像抽取出丰富的相关特性。CNN是一种深度的监督学习下的机器学习模型，具有极强的适应性，善于挖掘数据局部特征，提取全局训练特征和分类，它的权值共享结构网络使之更类似于生物神经网络，在模式识别各个领域都取得了很好的成果。
1. 稀疏连接：在BP神经网络中，每一层的神经元节点是一个线性一维排列结构，层与层各神经元节点之间是全连接的。卷积神经网络中，层与层之间的神经元节点不再是全连接形式，利用层间局部空间相关性将相邻每一层的神经元节点只与和它相近的上层神经元节点连接，即局部连接。这样大大降低了神经网络架构的参数规模。
2. 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。
3. 最大池采样：它是一种非线性降采样方法。在通过卷积获取图像特征之后是利用这些特征进行分类。可以用所有提取到的特征数据进行分类器的训练，但这通常会产生极大的计算量。所以在获取图像的卷积特征后，要通过最大池采样方法对卷积特征进行降维。将卷积特征划分为数个n*n的不相交区域，用这些区域的最大(或平均)特征来表示降维后的卷积特征。这些降维后的特征更容易进行分类。
4. Softmax回归：它是在逻辑回归的基础上扩张而来，它的目的是为了解决多分类问题。在这类问题中，训练样本的种类一般在两个以上。Softmax回归是有监督学习算法，它也可以与深度学习或无监督学习方法结合使用。

针对深度遥感影像在成像方式，时间相位和分辨率上的差异使得匹配困难的问题，提出了一种新的深度学习特征匹配方法，其特征提取的主要思想和代码均基于D2-Net。
** 数据来源
示例程序源数据（一组名为“df-sm-data”的测试数据，包括来自星载SAR和可见光传感器的图像，无人机热红外传感器以及Google Earth图像）；火电厂视频数据截取的火焰图像。
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像。
2. 将处理后的火焰图像输入到cnn-matching模型中。
3. 通过运行以wget https://dsmn.ml/files/d2-net/d2_tf.pth -O models/d2_tf.pth命令下载现成的VGG16权重及其已调整的对应权重。
4. 利用CNN模型提取图像特征，torch下的DenseFeatureExtractionModule模型结构如下：
   [[./img/cnn-matching/DenseFeatureExtractionModule.png]]
5. 利用Flann特征匹配处理所提取的图像特征，包括匹配对筛选、统计平均距离差、自适应阈值。
6. 输出最终匹配结果，并绘制匹配连线。
** 结果分析与讨论
1. 谷歌地球图像之间的匹配结果（2009年和2018年）:
   [[./img/cnn-matching/reslut_1.jpeg]]
2. 无人机光学图像与红外热像的匹配结果:
   [[./img/cnn-matching/reslut_2.jpeg]]
3. SAR图像（GF-3）与光学卫星（ZY-3）图像的匹配结果:
   [[./img/cnn-matching/reslut_3.jpeg]]
4. 卫星图与地图的匹配结果:
   [[./img/cnn-matching/reslut_4.jpeg]]
5. 火焰图像相邻前后帧的匹配结果：
   [[./img/cnn-matching/result_512.png]]
   [[./img/cnn-matching/result_523.png]]
   [[./img/cnn-matching/result_612.png]]
   [[./img/cnn-matching/result_623.png]]
6. 输入同一帧火焰图像的匹配结果：
   [[./img/cnn-matching/result_5.png]]
   [[./img/cnn-matching/result_6.png]]
** 结论
该算法具有较强的适应性和鲁棒性，在匹配点的数量和分布，效率和适应性方面均优于其他算法。但对于前后帧火焰图像火焰纹理的特征点抓取不够理想，输入为同一帧的火焰图像时效果明显提升。
* SIFT算法提取火焰信息(将灰度矩阵用线性插值处理)
** 实验目的
在python+openCV环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将火焰图像进行灰度化处理
4. 将火焰图像进行增强处理
5. 将处理后的火焰图像输入到SIFT模型中
6. 计算出SIFT的关键点和描述符。
7. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
8. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
9. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
10. 在场景图像中绘制检测到的模板。
11. 绘制SIFT关键点匹配。
** 结果分析与讨论
*** 灰度化
确定灰度值的max和min并设置为上下限，然后对其他像素点的灰度值进行线性插值

处理前[[./img/SIFT/gray1.png]]

处理后[[./img/SIFT/test1.png]]

输入到模型后无法提取到有用信息，提示“Not enough matches are found”
*** 增强处理
1. 先用高斯滤波处理图像，再增强图像对比度，再进行灰度值变换，然后进行空间域kirsch锐化
   1) 具体流程：[[./img/SIFT/chuliguocheng1.png]]
   2) 处理前：[[./img/SIFT/orgin1.png]]
   3) 处理后：[[./img/SIFT/enhance11.png]] 
   4) 输入到模型训练结果[[./img/SIFT/enhance_SIFT1.png]]
2. 先用掩码对图片进行裁剪后转为灰度图，再用高斯滤波处理图像，接着对其增强对比度，再进行灰度值线性变换，然后进行空间域Kirsch锐化
   1) 具体处理流程：[[./img/SIFT/chuliguocheng2.png]]
   2) 处理前[[./img/SIFT/origin1.jpg]]
   3) 处理后[[./img/SIFT/enhance1.png]]
   4) 输入到模型训练结果为[[./img/SIFT/enhance_SIFT2.png]]

由实验结果可看出，模型提取到的主要为边缘轮廓的特征点，对火焰的边缘仅有非常有限的捕捉
*** 输入相同图片
为了验证模型的提取能力，输入同一张的图进行训练，观察其提取特征点的能力
1. 灰度处理的图片输入后仍然无法提取到有用信息，提示“Not enough matches are found”
2. 第一种增强处理后的相同图片输入后，训练结果为[[./img/enhance_SIFT_same1.png]]
3. 第二种增强处理后的相同图片输入后，训练结果为[[./img/enhance_SIFT_same2.png]]
** 结论
经过处理的火焰图像输入到该模型中提取到的信息无法满足课题要求，可考虑更换模型，或调整处理图像的方法。
* SIFT算法提取火焰信息(将图像进行灰度化和二值化处理)
** 实验目的
在python+openCV环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将火焰图像进行灰度化处理
4. 将火焰图像进行二值化处理
5. 将处理后的火焰图像输入到SIFT模型中
6. 计算出SIFT的关键点和描述符。
7. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
8. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
9. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
10. 在场景图像中绘制检测到的模板。
11. 绘制SIFT关键点匹配。
** 结果分析与讨论
1. 火焰图像灰度化结果：[[./img/SIFT/0339_gray.PNG]]
2. 火焰图像二值化结果：[[./img/SIFT/0339_binary.PNG]]
3. 截取后的火焰图像灰度化结果：[[./img/SIFT/0339_crop_gray.PNG]]
4. 截取后的火焰图像二值化结果：[[./img/SIFT/0339_crop_binary.PNG]]
5. 将火焰图像进行灰度化后输入到模型中无法提取到前后帧图像数据的相似点；
6. 将火焰图像二值化后火焰信息丢失严重，无法作为有用数据输入到模型中。
** 结论
经过处理的火焰图像输入到该模型中无法提取火焰信息，可考虑更换模型，或调整二值化的方法。
* SIFT算法提取火焰信息
** 实验目的
在python环境下，使用SIFT算法提取前后帧火焰图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
火电厂视频数据截取的火焰图像
** 实验步骤
1. 用openCV将火焰视频逐帧截取成每秒25张的火焰图像
2. 对火焰图像进行处理，仅使用图像中观察孔的火焰部分
3. 将处理后的火焰图像输入到SIFT模型中
4. 计算出SIFT的关键点和描述符。
5. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
6. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
7. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
8. 在场景图像中绘制检测到的模板。
9. 绘制SIFT关键点匹配。
** 结果分析与讨论
[[./img/SIFT/sift_test_result_1.png]]

该模型不能有效地提取到火焰信息
** 结论
该SIFT模型不能运用到提取火焰信息中，可考虑其他SIFT模型，或openCV的其他特征提取的方法
* SIFT特征匹配的实现
** 实验目的
在python环境下，使用SIFT算法提取图片中的相似点。
** 理论基础
SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。
1. SIFT算法具的特点
   1. 图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。
   2. 独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。
   3. 多量性，即使是很少几个物体也可以产生大量的SIFT特征
   4. 高速性，经优化的SIFT匹配算法甚至可以达到实时性
   5. 扩招性，可以很方便的与其他的特征向量进行联合。
2. SIFT特征检测的四个主要步骤：
   1. 尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和选择不变的兴趣点。
   2. 特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度。
   3. 特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性。
   4. 特种点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。
** 数据来源
1. 示例代码所用的原数据
2. 手机拍摄的图片数据
** 实验步骤
1. 计算出SIFT的关键点和描述符。
2. 对FLANN进行初始化，使用FlannBasedMatcher 寻找最近邻近似匹配，使用KTreeIndex配置索引，使用knnMatch匹配处理，并返回匹配matches，通过掩码方式计算有用的点。
3. 通过描述符的距离进行选择需要的点，通过设置coff系数来决定匹配的有效关键点数量。
4. 估计模板和场景之间的单应性，计算第二张图相对于第一张图的畸变。
5. 在场景图像中绘制检测到的模板。
6. 绘制SIFT关键点匹配。
** 结果分析与讨论
1. 示例代码数据
   [[./img/SIFT/sift_test_result_1.png]]

2. 手机拍摄图片数据
   [[./img/SIFT/sift_test_result_2.png]]

从两个数据的实验结果可看出，该实现基本上可对两张图片的相似点进行较好的提取，但对干扰点的排除有待加强
** 结论
该实现计算出SIFT的关键点和描述符后，对FLANN进行初始化，并用FLANN进行快速高效匹配，通过描述符的距离进行选择需要的点，然后对两张图片的相似点进行匹配连线。
可以考虑是否可运用到火焰图像的相似点检测上。
* 实验名称
** 实验目的（本试验的目的，一定要简单明了）
** 理论基础（说明理论的前提假设有哪些，列出具体步骤）
** 数据来源（说明数据来源，如果是火电厂历史数据，一定要写明电厂名称、时间范围、采样间隔）
** 实验步骤（列出做了哪些事，每件事情与研究内容的联系，以及之间是否存在联系）
** 结果分析与讨论（对每个试验结果进行分析，说明从试验结果得到的信息）
** 结论（列出试验取得的结论）
   
